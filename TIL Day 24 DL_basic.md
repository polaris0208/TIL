## 딥러닝의 개념
** 인공신경망(Artificial Neural Networks) 기반 머신러닝 **
* 다층 신경망 사용 - 자동으로 학습, 복잡한 문제 해결
* 중요한 패턴 추출, 예측,분류, 생성 등 다양한 작업 수행
> 중요한 패턴 : 작업 수행에 큰 영향을 주는 요소 - 가장 잘 동작하는 특징

1. 딥러닝의 특징
* 비선형 추론 가능
* 다층 구조 - 고차원 특징 학습
* 자동 특징 추출: 별도의 공학 과정이 필요 없음

2. 딥러닝의 역사와 활용 방안
- 역사와 발전
    - 1980s 역전파 알고리즘(Backpropagation) 제안, 다층 신경망 학습 가능
    - 2000s 대규모 데이터셋 등장, 딥러닝 연구 활성화
    - AlexNet, VGGNest, ResNet, 등 다양한 딥러닝 모델 개발, 혁신적인 성과
    <br>
- 인공지능-머신러닝-딥러닝의 관계
  - AI: 인간의 지능을 모방, 문제를 해결하는 기술 / 규칙기반 학습~자율 학습 시스템
  - ML: 데이터를 이용해 모델을 학습하고 예측이나 결정을 내리는 기술
  - DL: 다층신경망 사용, 대규모 데이터 활용 복잡한 문제를 다룸
  <br>
- 최근의 할용 방안
  - 이미지 인식: 이미지 분류, 검출, 생성 - 자율 주행의 도로상황 인식
  - 자연어 처리: 번역, 요약, 감정분석 - 구글 번역기
  - 음성 인식
  - 의료 분야: 종합적

3. 배워야 하는 이유
* 높은 수요
* 혁신적인 성과
* 지속적인 발전
* 실용적인 응용
* **창의적인 가능성; 입력에 대한 제한이 줄어들면서 접근성이 높아짐 ; 다양한 가능성이 열림**

## 신경망의 기본원리
### 퍼셉트론(Perceptron)
- XOR 문제포함
1. 단일 퍼셉트론의 원리
  - 단일 퍼셉트론 개념  
    - Perceptron: 인공 신경망의 가장 기본적인 단위; 하나의 뉴런을 모델링
    - 입력값에 가중치(weight)를 곱하고 이를 모두 더한 후 활성화 함수(activateion function)을 통해 출력 값을 결정

2. 다층 퍼셉트론(MLP)
  - 다층 퍼셉트론의 개념
    - 여러층의 퍼셉트론을 쌓아올린 신경망 구조
  - 레이어의 개념
    - 입력층(input layer): 외부 데이터가 신경망에 입력되는 부분 / 입력 레이어의 뉴런 수 = 입력되는 특징 수
    - 은닉층(hidden layer): 입력 데이터를 처리하고 특징을 추출하는 역할 / 은닉층의 뉴런 수, 층 수는 모델의 복잡성과 층수에 영향
  - XOR 문제와 MLP
    - 단일 퍼셉트론은 선형 분류
    - XOR 문제는 두 입력 값이 다를 때만 1을 출력 - 단일 퍼셉트론으로는 대응 불가
    - MLP로는 비선형성을 해석 가능
### 활성화 함수 
1. 필요성
  - 입력값을 출력값으로 변환
  - 활성화 함수가 없으면 신경만은 단순 선형 변환만 수행, 복잡한 패턴 학습 불가
  - 비선형성 도입 복잡한 패턴 학습 가능
- 단점: 수학적 문제 발생

2. 활성화 함수의 종류
  - ReLU(Rectified Linear Unit) : 0과 x 사이의 max값 반환 계산이 가능
    - 기울기 소실문제 완화 / 죽은 ReLU 문제 발생: 음수 입력에 대해 기울기가 0이 됨
  - Sigmoid: 출력 값이 0과 1 사이로 제한 - 확률 표기에 적합
    - 기울기 소실 문제. 값이 0과 1에 가까워질 때 학습이 느려짐
  - Tanh (Hyperbolic Tangent):출력값 -1, 1사이 - 중심이 0애 가까어짐
    - 기울기 소실 문제

### 손실 함수와 최적화 알고리즘
- 용도에 따라 사용하는 종류가 다르다
1. 손실함수의 역할
  - 예측값과 실제값의 차이를 측정
  - 모델의 성능을 평가, 최적화 알고리즘을 통해 모델의 학습을 수행

2. 손실 함수의 종류
  - MSE: 회귀 문제
    - 에측값과 실제값의 차이를 제곱하여 평균을 구함
  - Cross-Entropy : 분류문제
    - 예측 확률과 실제 클래스 간의 차이를 측정
3. 최적화 알고리즘의 개념과 종류
  - 손실함수가 줄어들도록 가중치를 조절
  - 오차함수를 미분 해서 수렴하는 가중치를 찾음
  - local optimum 문제: 항상 최적값을 찾을 수 있는 것이 아님, 상황에 따라 
  - SGD(Stochastic Gradient Descent) 
    - 전체 데이터셋이 아닌 선택된 일부 데이터 사용하여 기울기 계산. 가중치 없데이트
    - 계산이 빠르고, 효율적
    - 최적점에 도달하기 전에 진동이 발생 가능
  - Adam(Adaptive Moment Estimation)
    - 모멘텀과 RMSProp을 결합한 알고리즘, 학습룰을 적응저으로 조정
    - 빠른 수렴속도, 안정적인 학습
    - 하이퍼 파라미터 설정이 복잡

### 역전파(Backpropagation)
1. 역전파 개념과 수학적 원리
- 역전파 알고리즘의 개념
  - 신경망의 가중치를 학습시키기 위한 알고리즘
  - 출력에서 입력 방향으로 손실함수의 기울기 계산, 가중치 업데이트
- 수학적 원리
  - 연쇄법칙(Chaine Rule) 여러개의 레이어의 기울기 계산하는 법칙
  - 각 층의 기울기는 이전 층의 기울기와 현재 층의 기울기를 곱하여 계산